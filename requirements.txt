# Core Dependencies for Qwen3-4B Fine-tuning Project
# ================================================

# Transformers & Model Loading
transformers>=4.40.0
accelerate>=0.28.0
safetensors>=0.4.0

# PEFT (LoRA/QLoRA)
peft>=0.10.0
bitsandbytes>=0.43.0

# Training (SFT & DPO)
trl>=0.8.0
datasets>=2.18.0

# Distributed Training
deepspeed>=0.14.0

# Evaluation
lm-eval>=0.4.0

# Utilities
numpy>=1.24.0
pandas>=2.0.0
pyyaml>=6.0
wandb>=0.16.0
tqdm>=4.65.0
einops>=0.7.0

# Tokenization
sentencepiece>=0.1.99
tiktoken>=0.6.0
