mlsys_project# MLSYS Project 1: 任务概览与规格说明书

本摘要涵盖了关于 Qwen3-4B 微调任务的所有关键信息，包括模型规格、硬件限制、项目阶段、提交要求及评估标准。

## 1. 简介与目标
该项目旨在让学生沉浸于现代大语言模型（LLM）微调的全生命周期中。
- **核心任务**：数据策展与预处理、监督微调（SFT）、强化学习（RL）、以及基于量规的基准测试评估。
- **重点难点**：计算预算规划、训练阶段的排序、以及交付物与评估标准的对齐。
- **挑战**：如何利用 SFT 和 RL 在这个较小的参数空间内“激发”出更强的逻辑推理和指令遵循能力。  

## 2. 核心项目组件

### 2.1 基础模型规格
- **模型名称**：Qwen3-4b-base
- **架构类型**：基于 Transformer 的稠密（dense）、仅解码器（decoder-only）因果 LLM。
- **参数量**：
  - 总参数：约 40 亿 (4.0B)
  - 非嵌入参数（核心推理）：约 36 亿 (3.6B)
- **层数**：36 层 Transformer。
- **注意力机制**：分组查询注意力（Grouped-Query Attention, GQA）。
  - 32 个查询头（query heads）。
  - 8 个键/值头（KV heads）——此设计旨在减少推理时的 KV 缓存显存并提高生成速度。
- **上下文长度**：原生 32,768 (32k) tokens；可通过 YaRN 等技术扩展至 131,072 (128k) 或更高。
- **嵌入层**：输入/输出嵌入权重共享（Tied embeddings），以最大化参数效率。

### 2.2 硬件约束
- **计算资源限制**：最多使用 4 张 H100 GPU。

### 2.3 模型已下载
- **路径**：基座模型 (Qwen3-4B) 已经预先下载到了 /afs/share/Qwen3-4B-Base 目录下

### 2.4 benchmark
- **说明**：课程提供 lm eval 的 benchmark：mmlu,hellaswag,arc_challenge,winogrande,piqa,gsm8k,humaneval,ifeval,commonsense_qa，
路径为/data/share/benchmark_cache
用的时候设置 HF_HOME，和HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1

## 3. 项目阶段与时间线
项目分为三个阶段，**每个阶段有严格的 24 小时训练时间限制**。

1.  **阶段 1：热身 (Warm-up)**
    - 权重：占最终分数的 **10%**。主要是构思初步的project实现方案，包括数据集选择、微调策略等。
2.  **阶段 2：监督微调 (SFT)**
    - 权重：占最终分数的 **40%**。首先需要对基座模型（Qwen3-4B-Base）进行监督微调（SFT）。这一步是为了让模型学会遵循指令和特定的对话格式。
3.  **阶段 3：强化学习 (RL)**
    - 权重：占最终分数的 **50%**。在 Stage 2 获得的 SFT 模型基础上，进行**强化学习（RL）**训练。这一步是为了进一步对齐人类价值观或优化特定指标（如评分标准中的 Benchmarks）。

## 4. 提交要求
- **最终模型 Checkpoint**：完成所有训练阶段后的完整模型文件
- **训练文档 (PDF)**：详细记录过程、方法和结果的报告。
- **代码仓库**：上传到github。

## 5. 评估标准与基准测试 (Benchmarks)
模型性能将通过五个关键能力集群进行评估。

### 5.1 广泛的学术与世界知识 (Broad Academic & World Knowledge)
- **基准测试**：MMLU, ARC。
- **能力描述**：测试模型作为“博学者”的能力，即在 STEM、人文和社会科学等领域存储、检索和应用事实信息的能力。
- **重要性**：证明模型不仅仅是模仿语言模式，而是“阅读”并理解了大量人类文献和科学原理。

### 5.2 常识与上下文推理 (Commonsense & Contextual Reasoning)
- **基准测试**：HellaSwag, WinoGrande, CSQA。
- **能力描述**：评估直觉智力，包括理解不成文的社会规则、解决语言歧义（如模糊代词）和预测日常事件。
- **重要性**：AI 需要深厚的文化和情境基础才能理解人类理所当然的事情（例如：切洋葱流泪是生理反应而非情感反应）。

### 5.3 物理推理与动力学 (Physical Reasoning & Dynamics)
- **基准测试**：PIQA, ARC, WinoGrande (部分)。
- **能力描述**：测试模型对现实世界物理（朴素物理学）的理解，如重力、材料属性（玻璃易碎、木头可燃）及物体交互。
- **重要性**：对于与现实世界交互（如机器人技术）或提供实用建议至关重要，避免产生违反物理规律的幻觉。

### 5.4 数学与算法逻辑 (Mathematical & Algorithmic Logic)
- **基准测试**：GSM8K, Human Eval。
- **能力描述**：衡量思维链（Chain-of-Thought, CoT）推理能力。要求模型执行多步逻辑推演、算术计算和算法规划，且过程中不迷失方向。
- **重要性**：区分“健谈”机器人与“智能”机器人的关键。证明模型能处理只有唯一正确答案的严格逻辑问题。

### 5.5 指令遵循与鲁棒性 (Instruction Following & Robustness)
- **基准测试**：IFEval。
- **具体指标**：
  - **指令遵循**：遵守特定格式（如 JSON、表格）和约束条件。
  - **指令鲁棒性**：处理 50 条“陷阱”指令（包括歧义、安全问题、逻辑冲突）。
- **能力描述**：评估模型在遵循复杂指令（包括格式要求）以及处理棘手或冲突提示时的表现。
- **重要性**：确保模型在实际应用中能被信任，准确且安全地遵循指导方针。